

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Step-by-step example for testing ryu with OpenStack &mdash; Ryu 1.8 documentation</title>
    
    <link rel="stylesheet" href="_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/print.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.8',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/theme_extras.js"></script>
    <link rel="top" title="Ryu 1.8 documentation" href="index.html" />
    <link rel="up" title="OpenStack Integration" href="openstack.html" />
    <link rel="next" title="Ryu L2 isolation" href="internals_l2_isolation.html" />
    <link rel="prev" title="Using Ryu Network Operating System with OpenStack as OpenFlow controller" href="using_with_openstack.html" /> 
  </head>
  <body>
      <div class="header"><h1 class="heading"><a href="index.html">
          <span>Ryu 1.8 documentation</span></a></h1>
        <h2 class="heading"><span>Step-by-step example for testing ryu with OpenStack</span></h2>
      </div>
      <div class="topnav">
      
        <p>
        «&#160;&#160;<a href="using_with_openstack.html">Using Ryu Network Operating System with OpenStack as OpenFlow controller</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="internals_l2_isolation.html">Ryu L2 isolation</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="step-by-step-example-for-testing-ryu-with-openstack">
<h1>Step-by-step example for testing ryu with OpenStack<a class="headerlink" href="#step-by-step-example-for-testing-ryu-with-openstack" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Here is the step-by-step to test if ryu plugin/segregation works with openstack.
In this example,</p>
<ol class="arabic simple">
<li>create one user account as an admin and an user</li>
<li>create two projects and create a network tenant for each project</li>
<li>run VM instances for each projects</li>
<li>open vga console via virt-manager</li>
<li>try to ping to each VMs</li>
</ol>
<p>Note: in this section, nova/quantum/ryu installation isn&#8217;t explained.
If you don&#8217;t have any experience with openstack nova, it is strongly
recommended to try plain nova and quantum with ovs plugin.</p>
</div>
<div class="section" id="conventions">
<h2>Conventions<a class="headerlink" href="#conventions" title="Permalink to this headline">¶</a></h2>
<p>The following variable is used to show values which depends on the
configuration.</p>
<ul>
<li><dl class="first docutils">
<dt>$username: nova user account name which is used as admin and user</dt>
<dd><p class="first">Probably you man want to create two account to separate admin
and user. In this example, only single account is used for
simplicity.</p>
<p class="last">e.g. yamahata</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>$tenant0: nova project name and tenant name.</dt>
<dd><p class="first">This name is used as both nova project name and nova network
tenant name.
Here we abuse nova project name as network tenant name for
simplicity. If you&#8217;d like to more complex setting, please refer
to nova documentation.</p>
<p class="last">e.g. yamahata-project-0</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>$iprange0: IP ranges which is used for $tenant0</dt>
<dd><p class="first last">e.g. 172.17.220.0/25</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>$tenant1: another project name</dt>
<dd><p class="first last">e.g. yamahata-project-1</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>$iprange1: another IP ranges for $tenant1</dt>
<dd><p class="first last">e.g. 172.17.221.0/25</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="step-by-step-testing">
<h2>step-by-step testing<a class="headerlink" href="#step-by-step-testing" title="Permalink to this headline">¶</a></h2>
<p>In this example, euca2ools is used because it&#8217;s handy.
The more openstack way is possible, though.</p>
<ol class="arabic">
<li><p class="first">setup nova data base</p>
<p>Run the following on a nova node:</p>
<div class="highlight-python"><pre>$ sudo nova-manage db sync</pre>
</div>
</li>
<li><p class="first">setup quantum data base</p>
<p>Use mysql command to connect mysql server:</p>
<div class="highlight-python"><pre>$ mysql -u &lt;admin user name&gt; -p</pre>
</div>
<p>Then create the quantum db and allow the agents to access it:</p>
<div class="highlight-python"><pre>mysql&gt; CREATE DATABASE ovs_quantum;
mysql&gt; GRANT USAGE ON *.* to &lt;user name&gt;@'yourremotehost' IDENTIFIED BY 'newpassword';
mysql&gt; FLUSH PRIVILEGES;</pre>
</div>
<p>Where the database name, ovs_quantum, the user name, &lt;user name&gt;, and
its password, newpassword, are the one defined in the ryu plugin
configuration file, ryu.ini.</p>
<p>If you are using multiple compute nodes, the GRANT sentence needs to
be repeated. Or wildcard, %, can be used like:</p>
<div class="highlight-python"><pre>mysql&gt; GRANT USAGE ON *.* to &lt;user name&gt;@'%' IDENTIFIED BY 'newpassword';</pre>
</div>
</li>
<li><p class="first">Make sure all nova, quantum, ryu and other openstack components are
installed and running</p>
<p>Especially</p>
<ul class="simple">
<li>On nova compute/network node<ul>
<li>Ryu must be installed</li>
<li>ryu quantum agent(ryu_quantum_agent.py) is put somewhere and
it must be running</li>
<li>ovs bridge is configured</li>
</ul>
</li>
<li>on machine quantum-server is running<ul>
<li>Ryu must be installed</li>
</ul>
</li>
<li>the db server is accessible from all related servers</li>
</ul>
</li>
<li><p class="first">create a user on a nova node</p>
<p>Run the following on a nova node:</p>
<div class="highlight-python"><pre>$ sudo nova-manage --flagfile=/etc/nova/nova.conf user admin $username</pre>
</div>
</li>
<li><p class="first">Create project, get the zipfile for the project, unextract it and create</p>
<p>ssh key for $tenant0
Run the following:</p>
<div class="highlight-python"><pre>$ sudo nova-manage --flagfile /etc/nova/nova.conf project create $tenant0 --user=$username
$ sudo nova-manage --flagfile=/etc/nova/nova.conf project create $tenant0 $username ./$tenant0.zip
$ sudo unzip ./$tenant0.zip -d $tenant0
$ source ./$tenant0/novarc
$ euca-add-keypair mykey-$tenant0 &gt; mykey-$tenant0.priv</pre>
</div>
</li>
<li><p class="first">do the same of the above step for $tenant1</p>
</li>
<li><p class="first">create networks for each projects</p>
<p>Run the followings:</p>
<div class="highlight-python"><pre>$ sudo nova-manage --flagfile=/etc/nova/nova.conf network create --label=$tenant0 --fixed_range_v4=$iprange0 --project_id=$tenant0
$ sudo nova-manage --flagfile=/etc/nova/nova.conf network create --label=$tenant1 --fixed_range_v4=$iprange1 --project_id=$tenant1</pre>
</div>
</li>
<li><p class="first">register image file</p>
<p>Get the vm image from somewhere (or create it by yourself) and register it.
The easiest way is to get the image someone has already created. You can find
links from the below.</p>
<ul class="simple">
<li><a class="reference external" href="http://wiki.openstack.org/GettingImages">Getting Images that Work with OpenStack</a>.</li>
<li><a class="reference external" href="http://smoser.brickies.net/ubuntu/ttylinux-uec/">ttylinux by Scott Moser</a>.</li>
</ul>
<p>In this example we use the ttylinux image just because its size is small:</p>
<div class="highlight-python"><pre>$ wget http://smoser.brickies.net/ubuntu/ttylinux-uec/ttylinux-uec-i686-12.1_2.6.35-22_1.tar.gz
$ cloud-publish-tarball ttylinux-uec-i686-12.1_2.6.35-22_1.tar.gz &lt;bucket-name&gt;
$ euca-register &lt;bucket-name&gt;/ttylinux-uec-amd64-12.1_2.6.35-22_1.img.manifest.xml</pre>
</div>
<p>Now you get the image id, ari-xxx, aki-xxx and ami-xxx, where xxx is
replaced with some id number.
Depending on which distribution you use, you need to use other command like
uec-publish-tarball.
If you customize images, you may have to use commands like euca-bundle-image,
euca-upload-image, euca-register.</p>
<p>Or you want to go for more openstack way, glance command is your friend
to create/register image.</p>
</li>
<li><p class="first">run instances</p>
<p>boot instances for each projects.
In order to test network segregation, 2 or more VM instances need to
be created:</p>
</li>
</ol>
<div class="highlight-python"><pre>$ source ./$tenant0/novarc
$ euca-run-instances ami-&lt;id which you get above&gt; -k mykey-$tenant0 -t m1.tiny
# repeat euca-run-instances for some times.
$ source ./$tenant1/novarc
$ euca-run-instances ami-&lt;id which you get above&gt; -k mykey-$tenant1 -t m1.tiny</pre>
</div>
<ol class="arabic">
<li><p class="first">check if VM instances are created</p>
<p>Get the list of VM instances you&#8217;ve created and their assigned IP address:</p>
<div class="highlight-python"><pre>$ euca-describe-instances</pre>
</div>
</li>
<li><p class="first">login VM instances and try ping/traceroute</p>
<p>In plain nova case, you can login the VM instances by ssh like
&#8220;ssh -i mykey-$tenant0.priv <a class="reference external" href="mailto:root&#37;&#52;&#48;$ipaddress">root<span>&#64;</span>$ipaddress</a>&#8221;
However, the VM instances are segregated from the management network. So the
story differs. the easiest way to login the VM is to use virt-manager
(or virsh) on each compute nodes.
Identify on which compute node the VM is running by euca-describe-instances,
and run virt-manager on the compute node. Show the vga console by
virt-manager GUI, then you can login the VM instances.</p>
<p>Then try &#8220;ping &lt;other VM IP or gateway&gt;&#8221; or &#8220;traceroute &lt;ip address&gt;&#8221;
on each consoles.</p>
</li>
<li><p class="first">packet capture (optional)</p>
<p>You can run wireshark or similar tools in order to observe what packets
are sent.</p>
</li>
</ol>
</div>
<div class="section" id="when-something-goes-wrong">
<h2>When something goes wrong<a class="headerlink" href="#when-something-goes-wrong" title="Permalink to this headline">¶</a></h2>
<p>Something can go wrong sometimes unfortunately.
Database tables used by openstack nova/quantum seems very fragile.
Db can result in broken state easily. If you hit it, the easiest way is</p>
<ol class="arabic">
<li><p class="first">stop all the related daemons</p>
</li>
<li><p class="first">drop related DB and re-create them.</p>
</li>
<li><p class="first">clean up OVS related stuff</p>
<p>OVS uses its own data base which is persistent. So reboot doesn&#8217;t fix it.
The leaked resources must be released explicitly by hand.
The following command would help.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># ip link delete &lt;tapxxx&gt;</span>
<span class="c"># tunctl -d &lt;tapxxx&gt;</span>
<span class="c"># ovs-vsctl del-port &lt;br-int&gt; &lt;gw-xxx&gt;</span>
<span class="c"># ovs-vsctl del-port &lt;br-int&gt; &lt;tapxxx&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">restart the daemons</p>
</li>
<li><p class="first">set up from the scratch.</p>
</li>
</ol>
<p>Although you can fix it by issuing SQL manually, you have to know what you&#8217;re
doing with db tables.</p>
</div>
<div class="section" id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h2>
<div class="section" id="configuration-file-examples">
<h3>configuration file examples<a class="headerlink" href="#configuration-file-examples" title="Permalink to this headline">¶</a></h3>
<p>This section includes sample configuration files I use for convenience.
Some values need to be changed depending on your setup. For example
IP addresses/port numbers.</p>
<ul class="simple">
<li>/etc/nova/nova.conf for api, compute, network, volume, object-store and scheduler node</li>
</ul>
<p>Here is the nova.conf on which all nova servers are running:</p>
<div class="highlight-python"><pre>--verbose
    # For debugging

--logdir=/var/log/nova
--state_path=/var/lib/nova
--lock_path=/var/lock/nova
    # I set those three above for my preference.
    # You don't have to set them if the default works for you

--use_deprecated-auth=true
    # This depends on which authentication method you use.

--sql_connection=mysql://nova:nova@localhost/nova
    # Change this depending on how MySQL(or other db?) is setup

--dhcpbridge_flagfile=/etc/nova/nova.conf
--dhcpbridge=/usr/local/bin/nova-dhcpbridge
    # This path depends on where you install nova.

--fixed_range=172.17.220.0/16
    # You have to change this parameter depending on which IPs you uses

--network_size=128
    # This depends on which IPs you uses for one tenant

--network_manager=nova.network.quantum.manager.QuantumManager
--quantum_connection_host=127.0.0.1 # &lt;IP on which quantume server runs&gt;
    # Change this according to your set up

--connection_type=libvirt
--libvirt_type=kvm
--firewall_driver=quantum.plugins.ryu.nova.firewall.NopFirewallDriver
--libvirt_ovs_integration_bridge=br-int
--libvirt_vif_type=ethernet
--libvirt_vif_driver=quantum.plugins.ryu.nova.vif.LibvirtOpenVswitchOFPRyuDriver
--libvirt_ovs_ryu_api_host=&lt;ip address on which ryu is running&gt;:&lt;port&gt;
    # default 172.0.0.1:8080

--linuxnet_interface_driver=quantum.plugins.ryu.nova.linux_net.LinuxOVSRyuInterfaceDriver
--linuxnet_ovs_ryu_api_host=&lt;ip address on which ryu is running&gt;:&lt;port&gt;
    # default 172.0.0.1:8080
    # usually same to libvirt_ovs_ryu_api_host

--quantum_use_dhcp</pre>
</div>
<ul class="simple">
<li>/etc/nova/nova.conf on compute nodes</li>
</ul>
<p>I copied the above to compute node and modified it.
So it includes unnecessary values for network node. Since they don&#8217;t harm,
I didn&#8217;t scrub them.:</p>
<div class="highlight-python"><pre>--verbose

--logdir=/var/log/nova
--state_path=/var/lib/nova
--lock_path=/var/lock/nova

--use_deprecated_auth

--sql_connection=mysql://nova:nova@&lt;IP address&gt;/nova

--dhcpbridge_flagfile=/etc/nova/nova.conf
--dhcpbridge=/usr/bin/nova-dhcpbridge

--fixed_range=172.17.220.0/16
--network_size=128

--network_manager=nova.network.quantum.manager.QuantumManager
--quantum_connection_host=&lt;IP address on which quantum server is runniung&gt;
--connection_type=libvirt
--libvirt_type=kvm
--libvirt_ovs_integration_bridge=br-int
--libvirt_vif_type=ethernet
--libvirt_vif_driver=quantum.plugins.ryu.nova.vif.LibvirtOpenVswitchOFPRyuDriver
--libvirt_ovs_ryu_api_host=&lt;ip address on which ryu is running&gt;:&lt;port&gt;
--linuxnet_interface_driver=quantum.plugins.ryu.nova.linux_net.LinuxOVSRyuInterfaceDriver
--linuxnet_ovs_ryu_api_host=&lt;ip address on which ryu is running&gt;:&lt;port&gt;
--firewall_driver=quantum.plugins.ryu.nova.firewall.NopFirewallDriver
--quantum_use_dhcp

--rabbit_host=&lt;IP address on which rabbit mq is running&gt;
--glance_api_servers=&lt;IP address on which glance api server is running&gt;:&lt;port&gt;
--ec2_host=&lt;IP address on which ec2 api server is running&gt;
--osapi_host=&lt;IP address on which OpenStack api server is running&gt;
--s3_host=&lt;IP address on which S3 host is running&gt;
--metadata_host=&lt;IP address on which ec2 meta data sever is running&gt;</pre>
</div>
<ul class="simple">
<li>/etc/quantum/plugins.ini</li>
</ul>
<p>This file needs to be installed on which quantum-server is running.
This file defines which quantum plugin is used:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">PLUGIN</span><span class="p">]</span>
<span class="c"># Quantum plugin provider module</span>
<span class="n">provider</span> <span class="o">=</span> <span class="n">quantum</span><span class="o">.</span><span class="n">plugins</span><span class="o">.</span><span class="n">ryu</span><span class="o">.</span><span class="n">ryu_quantum_plugin</span><span class="o">.</span><span class="n">RyuQuantumPlugin</span>
</pre></div>
</div>
<ul class="simple">
<li>/etc/quantum/quantum.conf</li>
</ul>
<p>This file needs to be installed on which quantum-server is running.
A configuration file for quantum server. I use this file as is.</p>
<ul class="simple">
<li>/etc/quantum/plugins/ryu/ryu.ini</li>
</ul>
<p>This files needs to be installed on nova-compute node, nova-network node
and quantum-server node.
This file defines several setting ryu quantum plugin/agent uses:</p>
<div class="highlight-python"><pre>[DATABASE]
# This line MUST be changed to actually run the plugin.
# Example: sql_connection = mysql://root:nova@127.0.0.1:3306/ovs_quantum
#sql_connection = mysql://&lt;user&gt;:&lt;pass&gt;@&lt;IP&gt;:&lt;port&gt;/&lt;dbname&gt;
sql_connection = mysql://quantum:quantum@172.0.0.1:3306/ovs_quantum

[OVS]
integration-bridge = br-int

# openflow-controller = &lt;host IP address of ofp controller&gt;:&lt;port: 6633&gt;
# openflow-rest-api = &lt;host IP address of ofp rest api service&gt;:&lt;port: 8080&gt;
openflow-controller = &lt;IP address on which ryu-manager is running&gt;:&lt;port&gt;
      # default 127.0.0.1:6633
      # This corresponds to &lt;ofp_listen_host&gt;:&lt;ofp_listen_port&gt; in ryu.conf

openflow-rest-api = &lt;IP address on which ryu-manager is running&gt;:&lt;port&gt;
      # default 127.0.0.1:8080
      # This corresponds to &lt;wsapi_host&gt;:&lt;wsapi_port&gt; in ryu.conf</pre>
</div>
<ul class="simple">
<li>/etc/ryu/ryu.conf</li>
</ul>
<p>This file needs to be installed on which ryu-manager is running.
If you use default configurations, you don&#8217;t have to modify it.
Just leave it blank:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Sample configuration file</span>
<span class="p">[</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="c">#wsapi_host=&lt;hostip&gt;</span>
<span class="c">#wsapi_port=&lt;port:8080&gt;</span>
<span class="c">#ofp_listen_host=&lt;hostip&gt;</span>
<span class="c">#ofp_listen_port=&lt;port:6633&gt;</span>
</pre></div>
</div>
</div>
</div>
</div>


      </div>
      <div class="bottomnav">
      
        <p>
        «&#160;&#160;<a href="using_with_openstack.html">Using Ryu Network Operating System with OpenStack as OpenFlow controller</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="internals_l2_isolation.html">Ryu L2 isolation</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer">
        &copy; Copyright 2011, 2012 Nippon Telegraph and Telephone Corporation.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>