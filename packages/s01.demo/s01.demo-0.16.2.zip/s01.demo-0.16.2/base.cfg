[buildout]
index = http://pypi.python.org/simple
find-links = http://pypi.python.org/simple/lovely.buildouthttp
             http://pypi.python.org/simple/zc.buildout
             http://pypi.python.org/simple/setuptools
develop = .
parts = tmp log
        settings
        list crawl scrapy
        crawl-settings-file
        crawl-settings-and-overrides
        crawl-overrides
        crawl-non-spider
        test coverage-test coverage-report

versions = versions


[versions]
Twisted = 12.1.0
twisted = 12.1.0
lxml = 2.3
simplejson = 2.6.0


[tmp]
recipe = z3c.recipe.dev:mkdir
path = parts/tmp


[log]
recipe = z3c.recipe.dev:mkdir
path = parts/log


# setup default settings
[settings]
recipe = s01.scrapy:settings
target = ${buildout:directory}/parts/settings/default.cfg
mode = 0755
content =# scrapy settings for s01.demo package generated by buildout
  BOT_NAME = 'pypi'
  BOT_VERSION = '1.0'
  USER_AGENT = 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.01)'
  SPIDER_MODULES = ['s01.demo.spiders']
  LOG_ENABLED = True
  LOG_LEVEL = 'ERROR'
  LOG_FILE = '${buildout:directory}/parts/log/scrapy.log'
  DEFAULT_ITEM_CLASS = 's01.demo.item.DemoItem'
  ITEM_PIPELINES = ['s01.scrapy.exporter.TestExporter']
  S01_SCRAPY_TEST_EXPORT_DIR = '${buildout:directory}/parts/tmp'


# used by s01.worker for run spider
[crawl]
recipe = s01.scrapy:crawl
eggs = s01.demo
settings = settings


# used by s01.worker for list spiders
[list]
recipe = s01.scrapy:list
eggs = s01.demo
settings = settings


# generic scrapy script (not used by s01.worker)
[scrapy]
recipe = s01.scrapy:scrapy
eggs = s01.demo
settings = ${buildout:directory}/parts/settings/default.cfg


# sample using settings and overrides (not used by s01.worker)
[crawl-settings-file]
recipe = s01.scrapy:crawl
eggs = s01.demo
settings = ${buildout:directory}/parts/settings/default.cfg


# sample using settings and overrides (not used by s01.worker)
[crawl-settings-and-overrides]
recipe = s01.scrapy:crawl
eggs = s01.demo
settings = ${buildout:directory}/parts/settings/default.cfg
overrides = # adhoc scrapy settings which will not get dumped to any file
  BOT_NAME = 'pypi 2'
  BOT_VERSION = '4.0'
  USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)


# sample using only overrides (not used by s01.worker)
[crawl-overrides]
recipe = s01.scrapy:crawl
eggs = s01.demo
testing = true
overrides = # adhoc scrapy settings which will not get dumped to any file
  BOT_NAME = 'python.org'
  BOT_VERSION = '3.0'
  USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)
  SPIDER_MODULES = ['s01.demo.spiders']
  DEFAULT_ITEM_CLASS = 's01.demo.item.DemoItem'
  FEED_URI = 'file://${buildout:directory}/parts/tmp/output.csv'
  FEED_FORMAT = 'csv'


# sample using non spider name (not used by s01.worker)
[crawl-non-spider]
recipe = s01.scrapy:crawl
eggs = s01.demo
settings = ${buildout:directory}/parts/settings/default.cfg
