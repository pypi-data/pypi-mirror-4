

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Getting started &mdash; django-dynamic-scraper 0.2-alpha documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.2-alpha',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="django-dynamic-scraper 0.2-alpha documentation" href="index.html" />
    <link rel="next" title="Advanced topics" href="advanced_topics.html" />
    <link rel="prev" title="django-dynamic-scraper - Documentation" href="index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="advanced_topics.html" title="Advanced topics"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="django-dynamic-scraper - Documentation"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">django-dynamic-scraper 0.2-alpha documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>With Django Dynamic Scraper (DDS) you can define your <a class="reference external" href="http://www.scrapy.org">Scrapy</a> scrapers dynamically via the Django admin interface
and save your scraped items in the database you defined for your Django project.
Since it simplifies things DDS is not usable for all kinds of scrapers, but it is well suited for the relatively
common case of regularly scraping a website with a list of updated items (e.g. news, events, etc.) and than dig
into the detail page to scrape some more infos for each item.</p>
<p>Here are some examples for some use cases of DDS:
Build a scraper for ...</p>
<ul class="simple">
<li>Local music events for different event locations in your city</li>
<li>New organic recipes for asian food</li>
<li>The latest articles from blogs covering fashion and style in Berlin</li>
<li>...Up to your imagination! :-)</li>
</ul>
<p>Django Dynamic Scraper tries to keep its data structure in the database as separated as possible from the
models in your app, so it comes with its own Django model classes for defining scrapers, runtime information
related to your scraper runs and classes for defining the attributes of the models you want to scrape.
So apart from a few foreign key relations your Django models stay relatively independent and you don&#8217;t have
to adjust your model code every time DDS&#8217;s model structure changes.</p>
<p>The DDS repository on GitHub contains an example project in the <tt class="docutils literal"><span class="pre">example_project</span></tt> folder, showing how to
create a scraper for open news content on the web (starting with <a class="reference external" href="http://en.wikinews.org/wiki/Main_Page">Wikinews</a> from Wikipedia). The source code
from this example is used in the following guidelines.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">While there is a testsuite for DDS with tests for most of its&#8217; features and the app runs relatively stable
and is also used in production, DDS is <strong>alpha</strong> and still in an early development phase. Expect API changes
in future releases which will require manual adaption in your code. During alpha phase, API and/or DB changes
can also occur after minor release updates (e.g. from 0.2.0 to 0.2.1).</p>
</div>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="requirements">
<h3>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h3>
<p>The <strong>basic requirements</strong> for Django Dynamic Scraper are:</p>
<ul class="simple">
<li>Python 2.7+ (earlier versions untested)</li>
<li><a class="reference external" href="https://www.djangoproject.com/">Django</a> 1.4+ (earlier versions untested)</li>
<li><a class="reference external" href="http://www.scrapy.org">Scrapy</a> 0.14+ (necessary)</li>
</ul>
<p>If you want to use the <strong>scheduling mechanism</strong> of DDS you also have to install <tt class="docutils literal"><span class="pre">django-celery</span></tt> (there
is no separate <tt class="docutils literal"><span class="pre">django-kombu</span></tt> app in the newer version).
You should be able to use another messaging framework/store than <tt class="docutils literal"><span class="pre">django-kombu</span></tt>
but this is untested and <tt class="docutils literal"><span class="pre">django-kombu</span></tt> is the easiest choice which normally should be sufficient
for the simple purpose it is serving here.</p>
<ul class="simple">
<li><a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a> 2.5.5+ (earlier versions untested), Attention: Celery 3.x not yet supported!</li>
</ul>
<p>For <strong>scraping images</strong> you will also need the Python Image Library:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.pythonware.com/products/pil/">Python Image Libray (PIL) 1.1.7+</a> (earlier versions untested)</li>
</ul>
<p>And finally: <strong>Beginning with v.0.2</strong> DDS is using <tt class="docutils literal"><span class="pre">South</span></tt> for <strong>migrations in the DB schema</strong> between
different versions (e.g. if a new attribute is added to a model class). If you don&#8217;t exactly know what
<tt class="docutils literal"><span class="pre">South</span></tt> is and
what it does, it is highly recommended that you take the (relatively short) time to learn how to use it.
Since DDS is in an early development stage, it is very likely that the DB schema will change in the
future, and using <tt class="docutils literal"><span class="pre">South</span></tt> instead of <tt class="docutils literal"><span class="pre">syncdb</span></tt> to create and update your DB schema will make your
life a lot easier if you want to keep pace with the latest versions of DDS:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://south.aeracode.org/">South 0.7+</a> (earlier versions untested)</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please drop a note if you have tested DDS with older versions of the libraries above!</p>
</div>
</div>
<div class="section" id="installation-configuration">
<h3>Installation/Configuration<a class="headerlink" href="#installation-configuration" title="Permalink to this headline">¶</a></h3>
<p>For installing Django Dynamic Scraper you have to first met the requirements above. You can do this by
manually installing the libraries you need with <tt class="docutils literal"><span class="pre">pip</span></tt> or <tt class="docutils literal"><span class="pre">easy_install</span></tt>, which may be a better choice
if you e.g. don&#8217;t want to risk your Django installation to be touched during the installation process.
However if you are sure that there
is no danger ahead or if you are running DDS in a new <tt class="docutils literal"><span class="pre">virtualenv</span></tt> environment, you can install all the
requirements above together with:</p>
<div class="highlight-python"><pre>pip install -r requirements.txt</pre>
</div>
<p>Then download the DDS source code from GitHub and either add the <tt class="docutils literal"><span class="pre">dynamic_scraper</span></tt> folder to your
<tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt> or your project manually or install DDS with:</p>
<div class="highlight-python"><pre>python setup.py install</pre>
</div>
<p>Note, that the requirements are NOT included in the <tt class="docutils literal"><span class="pre">setup.py</span></tt> script since this caused some problems
when testing the installation and the requirements installation process with <tt class="docutils literal"><span class="pre">pip</span></tt> turned out to be
more stable.</p>
<p>Now, to use DDS in your Django project add <tt class="docutils literal"><span class="pre">'dynamic_scraper'</span></tt> to your <tt class="docutils literal"><span class="pre">INSTALLED_APPS</span></tt> in your
project settings.</p>
</div>
</div>
<div class="section" id="creating-your-django-models">
<h2>Creating your Django models<a class="headerlink" href="#creating-your-django-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="create-your-model-classes">
<h3>Create your model classes<a class="headerlink" href="#create-your-model-classes" title="Permalink to this headline">¶</a></h3>
<p>When you want to build a Django app using Django Dynamic Scraper to fill up your models with data you have
to provide <em>two model classes</em>. The <em>first class</em> stores your scraped data, in our news example this is a
class called <tt class="docutils literal"><span class="pre">Article</span></tt> storing articles scraped from different news websites.
The <em>second class</em> is a reference class for this first model class, defining where
the scraped items belong to. Often this class will represent a website, but it could also represent a
category, a topic or something similar. In our news example we call the class <tt class="docutils literal"><span class="pre">NewsWebsite</span></tt>. Below is the
code for this two model classes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">django.db</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">dynamic_scraper.models</span> <span class="kn">import</span> <span class="n">Scraper</span><span class="p">,</span> <span class="n">SchedulerRuntime</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib_exp.djangoitem</span> <span class="kn">import</span> <span class="n">DjangoItem</span>


<span class="k">class</span> <span class="nc">NewsWebsite</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">CharField</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">URLField</span><span class="p">()</span>
    <span class="n">scraper</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">Scraper</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">null</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">on_delete</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">SET_NULL</span><span class="p">)</span>
    <span class="n">scraper_runtime</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">SchedulerRuntime</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">null</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">on_delete</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">SET_NULL</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__unicode__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>


<span class="k">class</span> <span class="nc">Article</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">CharField</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">news_website</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">NewsWebsite</span><span class="p">)</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">TextField</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">URLField</span><span class="p">()</span>
    <span class="n">checker_runtime</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">SchedulerRuntime</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">null</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">on_delete</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">SET_NULL</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__unicode__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">title</span>


<span class="k">class</span> <span class="nc">ArticleItem</span><span class="p">(</span><span class="n">DjangoItem</span><span class="p">):</span>
    <span class="n">django_model</span> <span class="o">=</span> <span class="n">Article</span>
</pre></div>
</div>
<p>As you can see, there are some foreign key fields defined in the models referencing DDS models.
The <tt class="docutils literal"><span class="pre">NewsWebsite</span></tt> class has a reference to the <a class="reference internal" href="reference.html#scraper"><em>Scraper</em></a> DDS model, which contains the main
scraper with information about how to scrape the attributes of the article objects. The <tt class="docutils literal"><span class="pre">scraper_runtime</span></tt>
field is a reference to the <a class="reference internal" href="reference.html#scheduler-runtime"><em>SchedulerRuntime</em></a> class from the DDS models. An object of this class stores
scheduling information, in this case information about when to run a news website scraper for the next time.
The <tt class="docutils literal"><span class="pre">NewsWebsite</span></tt> class also has to provide the url to be used during the scraping process. You can either
use (if existing) the representative url field of the model class, which is pointing to the nicely-layouted
overview news page also visited by the user. In this case we are choosing this way with taking the <tt class="docutils literal"><span class="pre">url</span></tt>
attribute of the model class as the scrape url. However, it often makes sense to provide a dedicated <tt class="docutils literal"><span class="pre">scrape_url</span></tt>
(you can name the attribute freely) field for cases, when the representative url differs from the scrape url
(e.g. if list content is loaded via ajax, or if you want to use another format of the content - e.g. the rss
feed - for scraping).</p>
<p>The <tt class="docutils literal"><span class="pre">Article</span></tt> class to store scraped news articles also has a reference to the <a class="reference internal" href="reference.html#scheduler-runtime"><em>SchedulerRuntime</em></a> DDS
model class called <tt class="docutils literal"><span class="pre">checker_runtime</span></tt>. In this case the scheduling object holds information about the next
existance check (using the <tt class="docutils literal"><span class="pre">url</span></tt> field from <tt class="docutils literal"><span class="pre">Article</span></tt>) to evaluate if the news article
still exists or if it can be deleted (see <a class="reference internal" href="advanced_topics.html#item-checkers"><em>Defining item checkers</em></a>).</p>
<p>Last but not least: Django Dynamic Scraper uses the (still experimental (!)) <a class="reference external" href="https://scrapy.readthedocs.org/en/latest/topics/djangoitem.html">DjangoItem</a> class from Scrapy for
being able to directly store the scraped data into the Django DB. You can store the item class
(here: <tt class="docutils literal"><span class="pre">ArticleItem</span></tt>) telling Scrapy which model class to use for storing the data directly underneath the
associated model class.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For having a loose coupling between your runtime objects and your domain model objects you should declare
the foreign keys to the DDS objects with the <tt class="docutils literal"><span class="pre">blank=True,</span> <span class="pre">null=True,</span> <span class="pre">on_delete=models.SET_NULL</span></tt>
field options. This will prevent a cascading delete of your reference object as well as the associated
scraped objects when a DDS object is deleted accidentally.</p>
</div>
</div>
<div class="section" id="deletion-of-objects">
<h3>Deletion of objects<a class="headerlink" href="#deletion-of-objects" title="Permalink to this headline">¶</a></h3>
<p>If you delete model objects via the Django admin interface, the runtime objects are not
deleted as well. If you want this to happen, you can use Django&#8217;s
<a class="reference external" href="https://docs.djangoproject.com/en/dev/topics/db/models/#overriding-model-methods">pre_delete signals</a>
in your <tt class="docutils literal"><span class="pre">models.py</span></tt> to delete e.g. the <tt class="docutils literal"><span class="pre">checker_runtime</span></tt> when deleting an article:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@receiver</span><span class="p">(</span><span class="n">pre_delete</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pre_delete_handler</span><span class="p">(</span><span class="n">sender</span><span class="p">,</span> <span class="n">instance</span><span class="p">,</span> <span class="n">using</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="o">....</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">Article</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">instance</span><span class="o">.</span><span class="n">checker_runtime</span><span class="p">:</span>
            <span class="n">instance</span><span class="o">.</span><span class="n">checker_runtime</span><span class="o">.</span><span class="n">delete</span><span class="p">()</span>

<span class="n">pre_delete</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">pre_delete_handler</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="defining-the-object-to-be-scraped">
<h2>Defining the object to be scraped<a class="headerlink" href="#defining-the-object-to-be-scraped" title="Permalink to this headline">¶</a></h2>
<p>If you have done everything right up till now and even synced your DB :-) your Django admin should look
similar to the following screenshot below, at least if you follow the example project:</p>
<img alt="_images/screenshot_django-admin_overview.png" src="_images/screenshot_django-admin_overview.png" />
<p>Before being able to create scrapers in Django Dynamic Scraper you have to define which parts of the Django
model class you defined above should be filled by your scraper. This is done via creating a new
<a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a> in your Django admin interface and then adding several <a class="reference internal" href="reference.html#scraped-obj-attr"><em>ScrapedObjAttr</em></a>
datasets to it, which is done inline in the form for the <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a>. The attributes for the
object class have to be named like the attributes in your model class to be scraped. In our open news example
we want the title, the description, and the url of an Article to be scraped, so we add these attributes with
the corresponding names to the scraped obj class.</p>
<p>The reason why we are redefining these attributes here, is that we can later define x_path elements for each
of theses attributes dynamically in the scrapers we want to create. When Django Dynamic Scraper
is scraping items, the <strong>general workflow of the scraping process</strong> is as follows:</p>
<ul class="simple">
<li>The DDS scraper is scraping base elements from the overview page of items beeing scraped, with each base
element encapsulating an item summary, e.g. in our open news example an article summary containing the
title of the article, a screenshot and a short description. The encapsuling html tag often is a <tt class="docutils literal"><span class="pre">div</span></tt>,
but could also be a <tt class="docutils literal"><span class="pre">td</span></tt> tag or something else.</li>
<li>Then the DDS scraper is scraping the url from this item summary block, which leads to the detail page of the item</li>
<li>All the real item attributes (like a title, a description, a date or an image) are then scraped either from
within the item summary block on the overview page or from the detail page of the item. This can be defined later
when creating the scraper itself.</li>
</ul>
<p>To define which of the scraped obj attributes are just simple standard attributes to be scraped, which one
is the base attribute (this is a bit of an artificial construct) and which one is the url to be followed
later, we have to choose an attribute type for each attribute defined. There is a choice between the following
types (taken from <tt class="docutils literal"><span class="pre">dynamic_scraper.models.ScrapedObjAttr</span></tt>):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ATTR_TYPE_CHOICES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="s">&#39;S&#39;</span><span class="p">,</span> <span class="s">&#39;STANDARD&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s">&#39;T&#39;</span><span class="p">,</span> <span class="s">&#39;STANDARD (UPDATE)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s">&#39;B&#39;</span><span class="p">,</span> <span class="s">&#39;BASE&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s">&#39;U&#39;</span><span class="p">,</span> <span class="s">&#39;DETAIL_PAGE_URL&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s">&#39;I&#39;</span><span class="p">,</span> <span class="s">&#39;IMAGE&#39;</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">STANDARD</span></tt>, <tt class="docutils literal"><span class="pre">BASE</span></tt> and <tt class="docutils literal"><span class="pre">DETAIL_PAGE_URL</span></tt> should be clear by now, <tt class="docutils literal"><span class="pre">STANDARD</span> <span class="pre">(UPDATE)</span></tt> behaves like <tt class="docutils literal"><span class="pre">STANDARD</span></tt>,
but these attributes are updated with the new values if the item is already in the DB. <tt class="docutils literal"><span class="pre">IMAGE</span></tt> represents attributes which will
hold images or screenshots. So for our open news example we define a base attribute called &#8216;base&#8217; with
type <tt class="docutils literal"><span class="pre">BASE</span></tt>, two standard elements &#8216;title&#8217; and &#8216;description&#8217; with type <tt class="docutils literal"><span class="pre">STANDARD</span></tt>
and a url field called &#8216;url&#8217; with type <tt class="docutils literal"><span class="pre">DETAIL_PAGE_URL</span></tt>. Your definition form for your scraped obj class
should look similar to the screenshot below:</p>
<img alt="_images/screenshot_django-admin_add_scraped_obj_class.png" src="_images/screenshot_django-admin_add_scraped_obj_class.png" />
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you define an attribute as <tt class="docutils literal"><span class="pre">STANDARD</span> <span class="pre">(UPDATE)</span></tt> attribute and your scraper reads the value for this attribute from the detail page
of the item, your scraping process requires <strong>much more page requests</strong>, because the scraper has to look at all the detail pages
even for items already in the DB to compare the values. If you don&#8217;t use the update functionality, use the simple <tt class="docutils literal"><span class="pre">STANDARD</span></tt>
attribute instead!</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Though it is a bit of a hack: if you want to <strong>scrape items on a website not leading to detail pages</strong> you can do
this by defining another (non url) field as the <tt class="docutils literal"><span class="pre">DETAIL_PAGE_URL</span></tt> field, e.g. a title or an id. Make sure that this
field is unique since the <tt class="docutils literal"><span class="pre">DETAIL_PAGE_URL</span></tt> field is also used as an identifier for preventing double
entries in the DB and don&#8217;t use the <tt class="docutils literal"><span class="pre">from_detail_page</span></tt> option in your scraper definitions. It is also not possible
to use checkers with this workaround. However: it works, I even wrote a unit test for this hack! :-)</p>
</div>
</div>
<div class="section" id="defining-your-scrapers">
<h2>Defining your scrapers<a class="headerlink" href="#defining-your-scrapers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="general-structure-of-a-scraper">
<h3>General structure of a scraper<a class="headerlink" href="#general-structure-of-a-scraper" title="Permalink to this headline">¶</a></h3>
<p>Scrapers for Django Dynamic Scraper are also defined in the Django admin interface. You first have to give the
scraper a name and select the associated <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a>. In our open news example we call the scraper
&#8216;Wikinews Scraper&#8217; and select the <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a> named &#8216;Article&#8217; defined above.</p>
<p>The main part of defining a scraper in DDS is to create several scraper elements, each connected to a
<a class="reference internal" href="reference.html#scraped-obj-attr"><em>ScrapedObjAttr</em></a> from the selected <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a>. Each scraper element define how to extract
the data for the specific <a class="reference internal" href="reference.html#scraped-obj-attr"><em>ScrapedObjAttr</em></a> by following the main concepts of <a class="reference external" href="http://www.scrapy.org">Scrapy</a> for scraping
data from websites. In the fields named &#8216;x_path&#8217; and &#8216;reg_exp&#8217; an XPath and (optionally) a regular expression
is defined to extract the data from the page, following Scrapy&#8217;s concept of
<a class="reference external" href="http://readthedocs.org/docs/scrapy/en/latest/topics/selectors.html">XPathSelectors</a>. The &#8216;from_detail_page&#8217;
check box tells the scraper, if the data for the object attibute for the scraper element should be extracted
from the overview page or the detail page of the specific item. The fields &#8216;processors&#8217; and &#8216;processors_ctxt&#8217; are
used to define output processors for your scraped data like they are defined in Scrapy&#8217;s
<a class="reference external" href="http://readthedocs.org/docs/scrapy/en/latest/topics/loaders.html">Item Loader section</a>.
You can use these processors e.g. to add a string to your scraped data or to bring a scraped date in a
common format. More on this later. Finally, the &#8216;mandatory&#8217; check box is indicating whether the data
scraped by the scraper element is a necessary field. If you define a scraper element as necessary and no
data could be scraped for this element the item will be dropped. Note, that you always have to keep attributes
mandatory, if the corresponding attributes of your domain model class is a mandatory field, otherwise the
scraped item can&#8217;t be saved in the DB.</p>
</div>
<div class="section" id="creating-the-scraper-of-our-open-news-example">
<h3>Creating the scraper of our open news example<a class="headerlink" href="#creating-the-scraper-of-our-open-news-example" title="Permalink to this headline">¶</a></h3>
<p>Let&#8217;s use the information above in the context of our <a class="reference external" href="http://en.wikinews.org/wiki/Main_Page">Wikinews</a> example. Below you see a screenshot of an
html code extract from the <a class="reference external" href="http://en.wikinews.org/wiki/Main_Page">Wikinews</a> overview page like it is displayed by the developer tools in Google&#8217;s
Chrome browser:</p>
<img alt="_images/screenshot_wikinews_overview_page_source.png" src="_images/screenshot_wikinews_overview_page_source.png" />
<p>The next screenshot is from a news article detail page:</p>
<img alt="_images/screenshot_wikinews_detail_page_source.png" src="_images/screenshot_wikinews_detail_page_source.png" />
<p>We will use these code snippets in our examples.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you don&#8217;t want to manually create the necessary DB objects for the example project, you can also run
<tt class="docutils literal"><span class="pre">python</span> <span class="pre">manage.py</span> <span class="pre">loaddata</span> <span class="pre">open_news/open_news.json</span></tt> from within the <tt class="docutils literal"><span class="pre">example_project</span></tt> directory in your
favorite shell to have all the objects necessary for the example created automatically .</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The WikiNews site changes its code from time to time. I will try to update the example code and text in the
docs, but I won&#8217;t keep pace with the screenshots so they can differ slightly compared to the real world example.</p>
</div>
<p>1. First we have to define a base
scraper element to get the enclosing DOM elements for news item
summaries. On the <a class="reference external" href="http://en.wikinews.org/wiki/Main_Page">Wikinews</a> overview page all news summaries are enclosed by <tt class="docutils literal"><span class="pre">&lt;td&gt;</span></tt> tags with a class
called &#8216;l_box&#8217;, so <tt class="docutils literal"><span class="pre">//td[&#64;class=&quot;l_box&quot;]</span></tt> should do the trick. We leave the rest of the field for the
scraper element on default.</p>
<p>2. It is not necessary but just for the purpose of this example let&#8217;s scrape the title of a news article
from the article detail page. On an article detail page the headline of the article is enclosed by a
<tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> tag with an id named &#8216;firstHeading&#8217;. So <tt class="docutils literal"><span class="pre">//h1[&#64;id=&quot;firstHeading&quot;]/span/text()</span></tt> should give us the headline.
Since we want to scrape from the detail page, we have to activate the &#8216;from_detail_page&#8217; check box.</p>
<p>3. All the standard elements we want to scrape from the overview page are defined relative to the
base element. Therefore keep in mind to leave the trailing double slashes of XPath definitions.
We scrape the short description of a news item from within a <tt class="docutils literal"><span class="pre">&lt;span&gt;</span></tt> tag with a class named &#8216;l_summary&#8217;.
So the XPath is <tt class="docutils literal"><span class="pre">p/span[&#64;class=&quot;l_summary&quot;]/text()</span></tt>.</p>
<p>4. And finally the url can be scraped via the XPath <tt class="docutils literal"><span class="pre">span[&#64;class=&quot;l_title&quot;]/a/&#64;href</span></tt>. Since we only scrape
the path of our url with this XPath and not the domain, we have to use a processor for the first time to complete
the url. For this purpose there is a predefined processor called &#8216;pre_url&#8217;. You can find more predefined
processors in the <tt class="docutils literal"><span class="pre">dynamic_scraper.utils.processors</span></tt> module. &#8216;pre_url&#8217; is simply doing what we want,
namely adding a base url string to the scraped string. To use a processor, just write the function name
in the processor field. Processors can be given some extra information via the processors_ctxt field.
In our case we need the spefic base url our scraped string should be appended to. Processor context
information is provided in a dictionary like form: <tt class="docutils literal"><span class="pre">'processor_name':</span> <span class="pre">'context'</span></tt>, in our case:
<tt class="docutils literal"><span class="pre">'pre_url':</span> <span class="pre">'http://en.wikinews.org'</span></tt>. Together with our scraped string this will create
the complete url.</p>
<img alt="_images/screenshot_django-admin_scraper_1.png" src="_images/screenshot_django-admin_scraper_1.png" />
<img alt="_images/screenshot_django-admin_scraper_2.png" src="_images/screenshot_django-admin_scraper_2.png" />
<p>This completes our scraper. The form you have filled out should look similar to the screenshot above
(which is broken down to two rows due to space issues).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also <strong>scrape</strong> attributes of your object <strong>from outside the base element</strong> by using the <tt class="docutils literal"><span class="pre">..</span></tt> notation
in your XPath expressions to get to the parent nodes!</p>
</div>
</div>
<div class="section" id="create-the-domain-entity-reference-object-newswebsite-for-our-open-news-example">
<h3>Create the domain entity reference object (NewsWebsite) for our open news example<a class="headerlink" href="#create-the-domain-entity-reference-object-newswebsite-for-our-open-news-example" title="Permalink to this headline">¶</a></h3>
<p>Now - finally - we are just one step away of having all objects created in our Django admin.
The last dataset we have to add is the reference object of our domain, meaning a <tt class="docutils literal"><span class="pre">NewsWebsite</span></tt>
object for the Wikinews Website.</p>
<p>To do this open the NewsWebsite form in the Django admin, give the object a meaningful name (&#8216;Wikinews&#8217;),
assign the scraper and create an empty <a class="reference internal" href="reference.html#scheduler-runtime"><em>SchedulerRuntime</em></a> object with <tt class="docutils literal"><span class="pre">SCRAPER</span></tt> as your
<tt class="docutils literal"><span class="pre">runtime_type</span></tt>.</p>
<img alt="_images/screenshot_django-admin_add_domain_ref_object.png" src="_images/screenshot_django-admin_add_domain_ref_object.png" />
</div>
</div>
<div class="section" id="setting-up-scrapy-create-necessary-python-modules-for-your-app">
<h2>Setting up Scrapy/Create necessary python modules for your app<a class="headerlink" href="#setting-up-scrapy-create-necessary-python-modules-for-your-app" title="Permalink to this headline">¶</a></h2>
<p>Now after having created the Django models we want to scrape and having created the scraper and associated
objects in the database we have to set up Scrapy and get it to work together with the stuff we have created.
To get this going, we have to create a new Scrapy project, adjust some settings in the configuration and create
two short python module files, one with a spider class, inheriting from <a class="reference internal" href="reference.html#django-spider"><em>DjangoSpider</em></a>, and a finalising
pipeline for saving our scraped objects.</p>
<div class="section" id="setting-up-scrapy">
<h3>Setting up Scrapy<a class="headerlink" href="#setting-up-scrapy" title="Permalink to this headline">¶</a></h3>
<p>For getting <a class="reference external" href="http://www.scrapy.org">Scrapy</a> to work the recommended way to start a new Scrapy project normally is to create a directory
and template file structure with the <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">myscrapyproject</span></tt> command on the shell first.
However, there is (initially) not so much code to be written left and the directory structure
created by the <tt class="docutils literal"><span class="pre">startproject</span></tt> command cannot really be used when connecting Scrapy to the Django Dynamic Scraper
library. So the easiest way to start a new scrapy project is to just manually add the <tt class="docutils literal"><span class="pre">scrapy.cfg</span></tt>
project configuration file as well as the Scrapy <tt class="docutils literal"><span class="pre">settings.py</span></tt> file and adjust these files to your needs.
It is recommended to just create the Scrapy project in the same Django app you used to create the models you
want to scrape and then place the modules needed for scrapy in a sub package called <tt class="docutils literal"><span class="pre">scraper</span></tt> or something
similar. After finishing this chapter you should end up with a directory structure similar to the following
(again illustrated using the open news example):</p>
<div class="highlight-python"><pre>example_project/
        scrapy.cfg
        open_news/
                models.py # Your models.py file
                scraper/
                        settings.py
                        spiders.py
                        (checkers.py)
                        pipelines.py
                        (tasks.py)</pre>
</div>
<p>Your <tt class="docutils literal"><span class="pre">scrapy.cfg</span></tt> file should look similar to the following, just having adjusted the reference to the
settings file and the project name:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">open_news</span><span class="o">.</span><span class="n">scraper</span><span class="o">.</span><span class="n">settings</span>

<span class="p">[</span><span class="n">deploy</span><span class="p">]</span>
<span class="c">#url = http://localhost:6800/</span>
<span class="n">project</span> <span class="o">=</span> <span class="n">open_news</span>
</pre></div>
</div>
<p>And this is your <tt class="docutils literal"><span class="pre">settings.py</span></tt> file:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os.path</span>

<span class="n">PROJECT_ROOT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span> <span class="o">+</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">PROJECT_ROOT</span><span class="p">,</span> <span class="s">&#39;../../..&#39;</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">PROJECT_ROOT</span><span class="p">,</span> <span class="s">&#39;../..&#39;</span><span class="p">)]</span>

<span class="kn">from</span> <span class="nn">django.core.management</span> <span class="kn">import</span> <span class="n">setup_environ</span>
<span class="kn">import</span> <span class="nn">example_project.settings</span>
<span class="n">setup_environ</span><span class="p">(</span><span class="n">example_project</span><span class="o">.</span><span class="n">settings</span><span class="p">)</span>

<span class="n">BOT_NAME</span> <span class="o">=</span> <span class="s">&#39;open_news&#39;</span>
<span class="n">BOT_VERSION</span> <span class="o">=</span> <span class="s">&#39;1.0&#39;</span>

<span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;dynamic_scraper.spiders&#39;</span><span class="p">,</span> <span class="s">&#39;open_news.scraper&#39;</span><span class="p">,]</span>
<span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s">&#39;</span><span class="si">%s</span><span class="s">/</span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">BOT_NAME</span><span class="p">,</span> <span class="n">BOT_VERSION</span><span class="p">)</span>

<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">&#39;dynamic_scraper.pipelines.ValidationPipeline&#39;</span><span class="p">,</span>
    <span class="s">&#39;open_news.scraper.pipelines.DjangoWriterPipeline&#39;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">SPIDER_MODULES</span></tt> setting is referencing the basic spiders of DDS and our <tt class="docutils literal"><span class="pre">scraper</span></tt> package where
Scrapy will find the (yet to be written) spider module. For the <tt class="docutils literal"><span class="pre">ITEM_PIPELINES</span></tt> setting we have to
add (at least) two pipelines. The first one is the mandatory pipeline from DDS, doing stuff like checking
for the mandatory attributes we have defined in our scraper in the DB or preventing double entries already
existing in the DB (identified by the url attribute of your scraped items) to be saved a second time.</p>
</div>
<div class="section" id="adding-the-spider-class">
<h3>Adding the spider class<a class="headerlink" href="#adding-the-spider-class" title="Permalink to this headline">¶</a></h3>
<p>The main work left to be done in our spider class - which is inheriting from the <a class="reference internal" href="reference.html#django-spider"><em>DjangoSpider</em></a> class
of Django Dynamic Scraper - is to instantiate the spider by connecting the domain model classes to it
in the <tt class="docutils literal"><span class="pre">__init__</span></tt> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">dynamic_scraper.spiders.django_spider</span> <span class="kn">import</span> <span class="n">DjangoSpider</span>
<span class="kn">from</span> <span class="nn">open_news.models</span> <span class="kn">import</span> <span class="n">NewsWebsite</span><span class="p">,</span> <span class="n">Article</span><span class="p">,</span> <span class="n">ArticleItem</span>


<span class="k">class</span> <span class="nc">ArticleSpider</span><span class="p">(</span><span class="n">DjangoSpider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;article_spider&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_ref_object</span><span class="p">(</span><span class="n">NewsWebsite</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scraper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_object</span><span class="o">.</span><span class="n">scraper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scrape_url</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_object</span><span class="o">.</span><span class="n">url</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_runtime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_object</span><span class="o">.</span><span class="n">scraper_runtime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scraped_obj_class</span> <span class="o">=</span> <span class="n">Article</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scraped_obj_item_class</span> <span class="o">=</span> <span class="n">ArticleItem</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ArticleSpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="adding-the-pipeline-class">
<span id="adding-pipeline-class"></span><h3>Adding the pipeline class<a class="headerlink" href="#adding-the-pipeline-class" title="Permalink to this headline">¶</a></h3>
<p>Since you maybe want to add some extra attributes to your scraped items, DDS is not saving the scraped items
for you but you have to do it manually in your own item pipeline:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">django.db.utils</span> <span class="kn">import</span> <span class="n">IntegrityError</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
<span class="kn">from</span> <span class="nn">dynamic_scraper.models</span> <span class="kn">import</span> <span class="n">SchedulerRuntime</span>

<span class="k">class</span> <span class="nc">DjangoWriterPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;news_website&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spider</span><span class="o">.</span><span class="n">ref_object</span>

            <span class="n">checker_rt</span> <span class="o">=</span> <span class="n">SchedulerRuntime</span><span class="p">(</span><span class="n">runtime_type</span><span class="o">=</span><span class="s">&#39;C&#39;</span><span class="p">)</span>
            <span class="n">checker_rt</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;checker_runtime&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">checker_rt</span>

            <span class="n">item</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="n">spider</span><span class="o">.</span><span class="n">action_successful</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">spider</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Item saved.&quot;</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

        <span class="k">except</span> <span class="n">IntegrityError</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">spider</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span> <span class="n">log</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Missing attribute.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>The things you always have to do here is adding the reference object to the scraped item class and - if you
are using checker functionality - create the runtime object for the checker. You also have to set the
<tt class="docutils literal"><span class="pre">action_successful</span></tt> attribute of the spider, which is used internally by DDS when the spider is closed.</p>
</div>
</div>
<div class="section" id="running-testing-your-scraper">
<span id="running-scrapers"></span><h2>Running/Testing your scraper<a class="headerlink" href="#running-testing-your-scraper" title="Permalink to this headline">¶</a></h2>
<p>You can run/test spiders created with Django Dynamic Scraper from the command line similar to how you would run your
normal Scrapy spiders, but with some additional arguments given. The syntax of the DDS spider run command is
as following:</p>
<div class="highlight-python"><pre>scrapy crawl SPIDERNAME -a id=REF_OBJECT_ID
                        [-a do_action=(yes|no) -a run_type=(TASK|SHELL)
                        -a max_items_read={Int} -a max_items_save={Int}]</pre>
</div>
<ul class="simple">
<li>With <tt class="docutils literal"><span class="pre">-a</span> <span class="pre">id=REF_OBJECT_ID</span></tt> you provide the ID of the reference object items should be scraped for,
in our example case that would be the Wikinews <tt class="docutils literal"><span class="pre">NewsWebsite</span></tt> object, probably with ID 1 if you haven&#8217;t
added other objects before. This argument is mandatory.</li>
<li>By default, items scraped from the command line are not saved in the DB. If you want this to happen,
you have to provide <tt class="docutils literal"><span class="pre">-a</span> <span class="pre">do_action=yes</span></tt>.</li>
<li>With <tt class="docutils literal"><span class="pre">-a</span> <span class="pre">run_type=(TASK|SHELL)</span></tt> you can simulate task based scraper runs invoked from the
command line. This can be useful for testing, just leave this argument for now.</li>
<li>With <tt class="docutils literal"><span class="pre">-a</span> <span class="pre">max_items_read={Int}</span></tt> and <tt class="docutils literal"><span class="pre">-a</span> <span class="pre">max_items_save={Int}</span></tt> you can override the scraper settings for these
params.</li>
</ul>
<p>So, to invoke our Wikinews scraper, we have the following command:</p>
<div class="highlight-python"><pre>scrapy crawl article_spider -a id=1 -a do_action=yes</pre>
</div>
<p>If you have done everything correctly (which would be a bit unlikely for the first run after so many single steps,
but just in theory... :-)), you should get some output similar to the following, of course with other
headlines:</p>
<img alt="_images/screenshot_scrapy_run_command_line.png" src="_images/screenshot_scrapy_run_command_line.png" />
<p>In your Django admin interface you should now see the scraped articles listed on the article overview page:</p>
<img alt="_images/screenshot_django-admin_articles_after_scraping.png" src="_images/screenshot_django-admin_articles_after_scraping.png" />
<p>Phew.</p>
<p>Your first scraper with Django Dynamic Scraper is working. Not so bad! If you do a second run and there
haven&#8217;t been any new bugs added to the DDS source code in the meantime, no extra article objects should be added
to the DB. If you try again later when some news articles changed on the Wikinews overview page, the new
articles should be added to the DB.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Getting started</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#installation">Installation</a><ul>
<li><a class="reference internal" href="#requirements">Requirements</a></li>
<li><a class="reference internal" href="#installation-configuration">Installation/Configuration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#creating-your-django-models">Creating your Django models</a><ul>
<li><a class="reference internal" href="#create-your-model-classes">Create your model classes</a></li>
<li><a class="reference internal" href="#deletion-of-objects">Deletion of objects</a></li>
</ul>
</li>
<li><a class="reference internal" href="#defining-the-object-to-be-scraped">Defining the object to be scraped</a></li>
<li><a class="reference internal" href="#defining-your-scrapers">Defining your scrapers</a><ul>
<li><a class="reference internal" href="#general-structure-of-a-scraper">General structure of a scraper</a></li>
<li><a class="reference internal" href="#creating-the-scraper-of-our-open-news-example">Creating the scraper of our open news example</a></li>
<li><a class="reference internal" href="#create-the-domain-entity-reference-object-newswebsite-for-our-open-news-example">Create the domain entity reference object (NewsWebsite) for our open news example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#setting-up-scrapy-create-necessary-python-modules-for-your-app">Setting up Scrapy/Create necessary python modules for your app</a><ul>
<li><a class="reference internal" href="#setting-up-scrapy">Setting up Scrapy</a></li>
<li><a class="reference internal" href="#adding-the-spider-class">Adding the spider class</a></li>
<li><a class="reference internal" href="#adding-the-pipeline-class">Adding the pipeline class</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-testing-your-scraper">Running/Testing your scraper</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter">django-dynamic-scraper - Documentation</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="advanced_topics.html"
                        title="next chapter">Advanced topics</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/getting_started.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="advanced_topics.html" title="Advanced topics"
             >next</a> |</li>
        <li class="right" >
          <a href="index.html" title="django-dynamic-scraper - Documentation"
             >previous</a> |</li>
        <li><a href="index.html">django-dynamic-scraper 0.2-alpha documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Holger Drewes.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>