
add-job-flow-steps
******************


DESCRIPTION
===========

add-job-flow-steps adds new steps to a running job flow. A maximum of
256 steps are allowed in each job flow.

If your job flow is long-running (such as a Hive data warehouse) or
complex, you may require more than 256 steps to process your data. You
can bypass the 256-step limitation in various ways, including using
the SSH shell to connect to the master node and submitting queries
directly to the software running on the master node, such as Hive and
Hadoop. For more information on how to do this, go to Add More than
256 --steps to a Job Flow in the *Amazon Elastic MapReduce Developer's
Guide* .

A step specifies the location of a JAR file stored either on the
master node of the job flow or in Amazon S3. Each step is performed by
the main function of the main class of the JAR file. The main class
can be specified either in the manifest of the JAR or by using the
MainFunction parameter of the step.

Elastic MapReduce executes each step in the order listed. For a step
to be considered complete, the main function must exit with a zero
exit code and all Hadoop jobs started while the step was running must
have completed and run successfully.

You can only add steps to a job flow that is in one of the following
states: STARTING, BOOTSTRAPPING, RUNNING, or WAITING.


SYNOPSIS
========

   aws emr add-job-flow-steps
     --steps <value>
     --job-flow-id <value>


REQUIRED PARAMETERS
===================

"--steps"  (list)
   A list of  StepConfig to be executed by the job flow.

      (structure)
         Specification of a job flow step.

         "hadoop_jar_step"  (structure)
            Specifies the JAR file used for the job flow step.

            "main_class"  (string)
               The name of the main class in the specified Java file.
               If not specified, the JAR file should specify a Main-
               Class in its manifest file.

            "args"  (list of string)
               A list of command line arguments passed to the JAR
               file's main function when executed.

            "jar"  (string)
               A path to a JAR file run during the step.

            "properties"  (list)
               A list of Java properties that are set when the step
               runs. You can use these properties to pass key value
               pairs to your main function.

                  (structure)
                     A key value pair.

                     "value"  (string)
                        The value part of the identified key.

                     "key"  (string)
                        The unique identifier of a key value pair.

         "name"  (string)
            The name of the job flow step.

         "action_on_failure"  (string)
            Specifies the action to take if the job flow step fails.

   *Parameter Syntax*

      [
        {
          "hadoop_jar_step": {
            {
              "main_class": "string",
              "args":
                ["string", ...],
              "jar": "string",
              "properties":
                [
                  {
                    "value": "string",
                    "key": "string"
                  }
                  ...
                ]
            },
          "name": "string",
          "action_on_failure": "TERMINATE_JOB_FLOW"|"CANCEL_AND_WAIT"|"CONTINUE"
        }
        ...
      ]

"--job-flow-id"  (string)
   A string that uniquely identifies the job flow. This identifier is
   returned by run-job-flow and can also be obtained from  describe-
   job-flows .


OPTIONAL PARAMETERS
===================

None
