

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Advanced topics &mdash; django-dynamic-scraper 0.2-alpha documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.2-alpha',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="django-dynamic-scraper 0.2-alpha documentation" href="index.html" />
    <link rel="next" title="Basic services" href="basic_services.html" />
    <link rel="prev" title="Getting started" href="getting_started.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="basic_services.html" title="Basic services"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="getting_started.html" title="Getting started"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">django-dynamic-scraper 0.2-alpha documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="advanced-topics">
<h1>Advanced topics<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h1>
<div class="section" id="defining-item-checkers">
<span id="item-checkers"></span><h2>Defining item checkers<a class="headerlink" href="#defining-item-checkers" title="Permalink to this headline">¶</a></h2>
<p>Django Dynamic Scraper comes with a built-in mechanism to check, if items once scraped are still existing
or if they could be deleted from the database. The entity providing this mechanism in DDS is called an
item checker. An item checker is like a scraper also using the scraping logic from Scrapy. But instead of
building together a new scraped item, it just checks the detail page referenced by the url of a scraped item.
Depending on the <tt class="docutils literal"><span class="pre">checker_type</span></tt> and the result of the detail page check, the scraped item is kept or
will be deleted from the DB.</p>
<div class="section" id="creating-a-checker-class">
<h3>Creating a checker class<a class="headerlink" href="#creating-a-checker-class" title="Permalink to this headline">¶</a></h3>
<p>To get a checker up and running you first have to create a checker class for each of your scraped object domain
models. In our open news example, this would be a class called <tt class="docutils literal"><span class="pre">ArticleChecker</span></tt> in a module called <tt class="docutils literal"><span class="pre">checkers</span></tt>
in our <tt class="docutils literal"><span class="pre">scraper</span></tt> directory:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">dynamic_scraper.spiders.django_checker</span> <span class="kn">import</span> <span class="n">DjangoChecker</span>
<span class="kn">from</span> <span class="nn">open_news.models</span> <span class="kn">import</span> <span class="n">Article</span>


<span class="k">class</span> <span class="nc">ArticleChecker</span><span class="p">(</span><span class="n">DjangoChecker</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;article_checker&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_ref_object</span><span class="p">(</span><span class="n">Article</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scraper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_object</span><span class="o">.</span><span class="n">news_website</span><span class="o">.</span><span class="n">scraper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scrape_url</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_object</span><span class="o">.</span><span class="n">url</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_runtime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_object</span><span class="o">.</span><span class="n">checker_runtime</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ArticleChecker</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>The checker class inherits from the <a class="reference internal" href="reference.html#django-checker"><em>DjangoChecker</em></a> class from DDS and mainly gives the checker the
information what to check and what parameters to use for checking. Be careful that the reference object
is now the scraped object itself, since the checker is scraping from the item page url of this object. The
url field to check is set with <tt class="docutils literal"><span class="pre">self.scrape_url</span> <span class="pre">=</span> <span class="pre">self.ref_object.url</span></tt>. Furthermore the checker needs its
configuration data from the scraper of the reference object. The scheduler runtime is used to schedule the
next check. So if you want to use checkers for your scraped object, you have to provide a foreign key to
a <a class="reference internal" href="reference.html#scheduler-runtime"><em>SchedulerRuntime</em></a> object in your model class. The scheduler runtime object also has to be saved
manually in your pipeline class (see: <a class="reference internal" href="getting_started.html#adding-pipeline-class"><em>Adding the pipeline class</em></a>).</p>
</div>
<div class="section" id="select-checker-type-set-check-parameters">
<h3>Select checker type/set check parameters<a class="headerlink" href="#select-checker-type-set-check-parameters" title="Permalink to this headline">¶</a></h3>
<p>There are momentarily the following checker types to choose from:</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="81%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">404</span></tt></td>
<td>Item is deleted after check has returned 404 HTTP status code 2x in a row</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">404_OR_X_PATH</span></tt></td>
<td>Same as 404 + check for an x_path value in the result</td>
</tr>
</tbody>
</table>
<p>The checker type and the x_path parameters when choosing <tt class="docutils literal"><span class="pre">404_OR_X_PATH</span></tt> as checker type
are defined in the Django admin forms of the different scrapers:</p>
<img alt="_images/screenshot_django-admin_checker_params.png" src="_images/screenshot_django-admin_checker_params.png" />
<p>For selecting a checker type and providing the parameters for an x_path checker
you have to look for an example item page url from the website
to be scraped which references an item not existing any more. If the urls to your scraped items are build
using an item ID you can e.g. try to lower this ID or increase it to a very large number. Be creative!
In our Wikinews example it is a bit different, since the news article url there is build using the title of the
article. So for the checker we take a random article url to a not existing article:
&#8220;<a class="reference external" href="http://en.wikinews.org/wiki/Random_article_text">http://en.wikinews.org/wiki/Random_article_text</a>&#8221;.</p>
<p>If your url found is responding with a 404 when invoked, you can simply choose <tt class="docutils literal"><span class="pre">404</span></tt> as your checker type.
For a <tt class="docutils literal"><span class="pre">404_OR_X_PATH</span></tt> checker you have to provide an XPath
for your chosen url which will extract a string from that url uniquely
indicating, that the content originally expected is not there any more. For our Wikinews example and the url
we choose above there is a text and a url provided suggesting to create the currently not existing wiki page,
so we can use the XPath <tt class="docutils literal"><span class="pre">//a[&#64;href=&quot;http://en.wikinews.org/wiki/This_wiki_article_doesnt_exist&quot;]/text()</span></tt>
and the result string &#8220;create this page&#8221; to uniquely identifying a scraped item not existing any more.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Attention! Make sure that the XPath/result string combination you choose is NOT succeeding on normal
item pages, otherwise the checker will delete all your items!</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To make sure your items aren&#8217;t deleted accidentally on a 404 response, 404 checks are only deleted on
the second try while XPath checks are deleted at once. So to save crawling resources always try to realize
your checking with XPath checks, otherwise the crawler need double the amount of checks!</p>
</div>
</div>
<div class="section" id="running-your-checkers">
<h3>Running your checkers<a class="headerlink" href="#running-your-checkers" title="Permalink to this headline">¶</a></h3>
<p>You can test your DDS checkers the same way you would run your scrapers:</p>
<div class="highlight-python"><pre>scrapy crawl CHECKERNAME -a id=REF_OBJECT_ID [-a do_action=(yes|no) -a run_type=(TASK|SHELL)]</pre>
</div>
<p>As a reference object ID you now have to provide the ID of a scraped item to be checked. With <tt class="docutils literal"><span class="pre">do_action=yes</span></tt>
an item is really deleted, otherwise the checker is only tested without actually manipulating the DB.</p>
<p>If you want to test a check on an item scraped in the open news example project, change the url of the item in
the DB to the checker reference url, look for the item ID and then run:</p>
<div class="highlight-python"><pre>scrapy crawl article_checker -a id=ITEM_ID -a do_action=yes</pre>
</div>
<p>If everything works well, your item should have been deleted.</p>
</div>
<div class="section" id="run-checker-tests">
<h3>Run checker tests<a class="headerlink" href="#run-checker-tests" title="Permalink to this headline">¶</a></h3>
<p>Django Dynamic Scraper comes with a build-in scraper called <tt class="docutils literal"><span class="pre">checker_test</span></tt> which can be used to test your checkers
against the defined reference url. You can run this checker on the command line with the following command:</p>
<div class="highlight-python"><pre>scrapy crawl checker_test -a id=SCRAPER_ID</pre>
</div>
<p>This scraper is useful both to look, if you have chosen a valid <tt class="docutils literal"><span class="pre">checker_x_path_ref_url</span></tt> and corresponding <tt class="docutils literal"><span class="pre">checker_x_path</span></tt>
and <tt class="docutils literal"><span class="pre">checker_x_path_result</span></tt> values as well as to see over time if your reference urls stay valid. For this use case
there exists a pre-defined celery task called <tt class="docutils literal"><span class="pre">run_checker_tests</span></tt> which can be used to run the checker test
for all the scrapers having a valid checker configuration. If a checker breaks over time (e.g. through a
renewed &#8220;Item not found&#8221; page) an error message will occur in the log table in the Django admin.</p>
</div>
</div>
<div class="section" id="scheduling-scrapers-checkers">
<h2>Scheduling scrapers/checkers<a class="headerlink" href="#scheduling-scrapers-checkers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>Django Dynamic Scraper comes with a build-in mechanism to schedule the runs of your scrapers as well as your
checkers. After each run DDS dynamically calculates the next execution time depending on the success of the run.
For a scraper that means, that the time between two scraper runs is shortened when new items could be scraped
from a page and is prolonged if not. For a checker, it means that a next check is prolonged if the check
was not successful, meaning that the item was not deleted. If it was deleted - well: than it was deleted!
No further action! :-) The parameters for this calculation (e.g. a min/max time period between two actions)
are defined for each <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a> in the DB.</p>
<p>DDS is using <a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a> to actually run your scrapers. Celery is a distributed task queue system for
Python, which means that you can run a celery daemon which takes task orders from somewhere and then executes
the corresponding tasks in a sequential way so that no task is lost, even if the system is under heavy load.
In our use case Celery is &#8220;just&#8221; working as a comfortable cron job replacement, which can be controlled via
the Django admin interface. The scheduler of DDS is using the scheduler runtime objects we defined for our
example scraper and checker in the sections before. The scheduler runtime objects contain some dynamic
information for the calculation of the next execution time of the scraper as well as the next execution time
itself. For django-celery a task for each <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a> has to be defined, which can than be
started and stopped in the Django admin interface. Each task is executed periodically in a configurable
time frame (e.g. ever hour). The task is then running all the scrapers associated with its <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a>,
which next execution time lies in the past. After each run, the next next execution time is calculated
by the scraper and saved into its scheduler runtime object. The next time this time lies in the past,
the scraper is run again.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The number of spiders/checkers run at each task run is limited by the <tt class="docutils literal"><span class="pre">DSCRAPER_MAX_SPIDER_RUNS_PER_TASK</span></tt>
and <tt class="docutils literal"><span class="pre">DSCRAPER_MAX_CHECKER_RUNS_PER_TASK</span></tt> settings which can be adopted in your custom settings file (see: <a class="reference internal" href="reference.html#settings"><em>Settings</em></a>).</p>
</div>
</div>
<div class="section" id="installing-configuring-django-celery-for-dds">
<h3>Installing/configuring django-celery for DDS<a class="headerlink" href="#installing-configuring-django-celery-for-dds" title="Permalink to this headline">¶</a></h3>
<p>This paragraph is covering only the specific installation issues with <a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a> in regard of installing
it for the use with DDS, so you should be familiar with the basic functionality of Celery and take general
installation infos from the <a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a> website. If you have successfully installed and configured
django-celery, you should see the <tt class="docutils literal"><span class="pre">Djcelery</span></tt> tables in the Django admin interface:</p>
<img alt="_images/screenshot_django-admin_overview.png" src="_images/screenshot_django-admin_overview.png" />
<p>For <tt class="docutils literal"><span class="pre">django-celery</span></tt> to work, Celery also needs a message broker for the actual message transport. For our
relatively simple use case, <a class="reference external" href="http://pypi.python.org/pypi/kombu">kombu</a> is the easiest and recommended choice. So please install kombu
as well.</p>
<p>Then we can configure <a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a> in combination with <a class="reference external" href="http://pypi.python.org/pypi/kombu">kombu</a> in our <tt class="docutils literal"><span class="pre">settings.py</span></tt> file. A starter
configuration could look similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># django-celery settings</span>
<span class="kn">import</span> <span class="nn">djcelery</span>
<span class="n">djcelery</span><span class="o">.</span><span class="n">setup_loader</span><span class="p">()</span>
<span class="n">BROKER_HOST</span> <span class="o">=</span> <span class="s">&quot;localhost&quot;</span>
<span class="n">BROKER_PORT</span> <span class="o">=</span> <span class="mi">5672</span>
<span class="n">BROKER_BACKEND</span> <span class="o">=</span> <span class="s">&quot;djkombu.transport.DatabaseTransport&quot;</span>
<span class="n">BROKER_USER</span> <span class="o">=</span> <span class="s">&quot;guest&quot;</span>
<span class="n">BROKER_PASSWORD</span> <span class="o">=</span> <span class="s">&quot;guest&quot;</span>
<span class="n">BROKER_VHOST</span> <span class="o">=</span> <span class="s">&quot;/&quot;</span>
<span class="n">CELERYBEAT_SCHEDULER</span> <span class="o">=</span> <span class="s">&#39;djcelery.schedulers.DatabaseScheduler&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">I had a lot of problems getting <strong>Celery</strong> to work properly, especially with <strong>conflicting library versions</strong>
when updating some parts of Celery. The following combination finally worked for me:
<cite>kombu 2.1.8</cite>, <cite>celery 2.5.3</cite>, <cite>django-celery 2.5.5</cite>. If you have problems with Celery, I would recommend
installing
exactly these library versions with e.g. <cite>pip install kombu==2.1.8</cite>. These problems are also the reason
I&#8217;m a bit cautious with supporting Celery 3.x, since I tend to wait a bit longer till all problems are
worked out. So there is <strong>no support for Celery 3.x yet</strong>!</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The usage of kombu in Django changed during the <tt class="docutils literal"><span class="pre">django-celery</span> <span class="pre">2.x</span></tt> trunk, there is no package
<tt class="docutils literal"><span class="pre">django-kombu</span></tt> any more and the configuration in <tt class="docutils literal"><span class="pre">settings.py</span></tt> changed.</p>
</div>
</div>
<div class="section" id="defining-your-tasks">
<h3>Defining your tasks<a class="headerlink" href="#defining-your-tasks" title="Permalink to this headline">¶</a></h3>
<p>For defining tasks for your scrapers and checkers which can be selected for periodical runs in the Django
admin interface, you have to define two short methods in a Python module in which your tasks are declared and make
sure, that your tasks are found by <a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a>. The easiest way to do this is by placing your methods in a
module called <tt class="docutils literal"><span class="pre">tasks.py</span></tt> in the main directory of your app. The tasks should then be found automatically.
The two methods in our open news example look like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">celery.task</span> <span class="kn">import</span> <span class="n">task</span>

<span class="kn">from</span> <span class="nn">dynamic_scraper.utils.task_utils</span> <span class="kn">import</span> <span class="n">TaskUtils</span>
<span class="kn">from</span> <span class="nn">open_news.models</span> <span class="kn">import</span> <span class="n">NewsWebsite</span><span class="p">,</span> <span class="n">Article</span>

<span class="nd">@task</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">run_spiders</span><span class="p">():</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">TaskUtils</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">run_spiders</span><span class="p">(</span><span class="n">NewsWebsite</span><span class="p">,</span> <span class="s">&#39;scraper&#39;</span><span class="p">,</span> <span class="s">&#39;scraper_runtime&#39;</span><span class="p">,</span> <span class="s">&#39;article_spider&#39;</span><span class="p">)</span>

<span class="nd">@task</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">run_checkers</span><span class="p">():</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">TaskUtils</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">run_checkers</span><span class="p">(</span><span class="n">Article</span><span class="p">,</span> <span class="s">&#39;news_website__scraper&#39;</span><span class="p">,</span> <span class="s">&#39;checker_runtime&#39;</span><span class="p">,</span> <span class="s">&#39;article_checker&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The two methods are decorated with the Celery task decorator to tell Celery that these methods should be
regarded as tasks. In each task, a method from the <tt class="docutils literal"><span class="pre">TaskUtils</span></tt> module from DDS is called to run the
spiders/checkers ready for the next execution.</p>
<p>Now you can create a peridoc task both for your scraper and your checker in the Django admin interface:</p>
<img alt="_images/screenshot_django-admin_peridoc_task.png" src="_images/screenshot_django-admin_peridoc_task.png" />
<p>In the peridoc task form you should be able to select your tasks defined above. Create an interval how often
these tasks are performed. In our open news example, 2 hours should be a good value. Please keep in mind, that
these are not the values how often a scraper/checker is actually run. If you define a two hour timeframe here,
it just means, that ever two hours, the task method executed is checking for scrapers/checkers with a next
execution time (defined by the associated <tt class="docutils literal"><span class="pre">scheduler_runtime</span></tt>) lying in the past and run these scrapers.
The actual time period between two runs is determined by the next execution time itself which is calculated
dynamically and depending on the scheduling configuration you&#8217;ll learn more about below. For the scrapers to
run, remember also that you have to set the scraper active in the associated <tt class="docutils literal"><span class="pre">scraper</span></tt> object.</p>
</div>
<div class="section" id="run-your-tasks">
<h3>Run your tasks<a class="headerlink" href="#run-your-tasks" title="Permalink to this headline">¶</a></h3>
<p>To actually run the task (respectively set our scheduling system to work as a whole) we have to run two different
daemon processes. The first one is the <tt class="docutils literal"><span class="pre">celeryd</span></tt> daemon from <a class="reference external" href="http://ask.github.com/django-celery/">django-celery</a> which is responsible for collecting
and executing tasks. We have to run <tt class="docutils literal"><span class="pre">celeryd</span></tt> with the -B option to also run the celerybeat
task scheduler which executes periodical tasks defined in Celery. Start the daemon with:</p>
<div class="highlight-python"><pre>python manage.py celeryd -l info -B --settings=example_project.settings</pre>
</div>
<p>If everything works well, you should now see the following line in your command line output:</p>
<div class="highlight-python"><pre>[2011-12-12 10:20:01,535: INFO/MainProcess] Celerybeat: Starting...</pre>
</div>
<p>As a second daemon process we need the server coming along with Scrapy to actually crawl the different
websites targeted with our scrapers. You can start the <a class="reference external" href="http://doc.scrapy.org/en/0.14/topics/scrapyd.html">Scrapy Server</a> with the following command from
within the main directory of your project:</p>
<div class="highlight-python"><pre>scrapy server</pre>
</div>
<p>You should get an output similar to the following:</p>
<img alt="_images/screenshot_shell_scrapy_server.png" src="_images/screenshot_shell_scrapy_server.png" />
<p>For testing your scheduling system, you can temporarily set your time interval of your periodic task to
a lower interval, e.g. 1 minute. Now you should see a new task coming in and being executed every minute:</p>
<div class="highlight-python"><pre>Got task from broker: open_news.tasks.run_spiders[5a3fed53-c26a-4f8f-b946-8c4a2c7c5c83]
Task open_news.tasks.run_spiders[5a3fed53-c26a-4f8f-b946-8c4a2c7c5c83] succeeded in 0.052549123764s: None</pre>
</div>
<p>The executed task should then run the scrapers/checkers which you should see in the output of the Scrapy
server:</p>
<div class="highlight-python"><pre>Process started: project='default' spider='article_spider' job='41f27199259e11e192041093e90a480a' pid=5932...
Process finished: project='default' spider='article_spider' job='41f27199259e11e192041093e90a480a' pid=5932...</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Note that you can vary the log level for debugging as well as other run parameters when you start
the servers, see the man/help pages of the celery and the Scrapy daemons.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please see this configuration described here just as a hint to get started. If you want to use
this in production you have to provide extra measures to make sure that your servers run constantly and that
they are secure. See the specific server documentation for more information.</p>
</div>
</div>
<div class="section" id="scheduling-configuration">
<h3>Scheduling configuration<a class="headerlink" href="#scheduling-configuration" title="Permalink to this headline">¶</a></h3>
<p>Now coming to the little bit of magic added to all this stuff with dynamic scheduling. The basis for the dynamic
scheduling in DDS is layed both for your scrapers and your checkers with the scheduling configuration parameters
in your scraped object class definitions in the Django admin interface. The default configuration for a
scraper looks like this:</p>
<div class="highlight-python"><pre>"MIN_TIME": 15,
"MAX_TIME": 10080,
"INITIAL_NEXT_ACTION_FACTOR": 10,
"ZERO_ACTIONS_FACTOR_CHANGE": 20,
"FACTOR_CHANGE_FACTOR": 1.3,</pre>
</div>
<p>Scheduling now works as follows: the inital time period between two scraper runs is calculated by taking the
product of the <tt class="docutils literal"><span class="pre">MIN_TIME</span></tt> and the <tt class="docutils literal"><span class="pre">INITIAL_NEXT_ACTION_FACTOR</span></tt>, with minutes as the basic time unit for
<tt class="docutils literal"><span class="pre">MIN_TIME</span></tt> and <tt class="docutils literal"><span class="pre">MAX_TIME</span></tt>:</p>
<div class="highlight-python"><pre>initial time period := 15 Minutes (MIN_TIME) * 10 (INITIAL_NEXT_ACTION_FACTOR) = 150 Minutes = 2 1/2 Hours</pre>
</div>
<p>Now, every time a scraper run was successful, the new next action factor is calculated by dividing the actual
next action factor by the <tt class="docutils literal"><span class="pre">FACTOR_CHANGE_FACTOR</span></tt>. So a successful scraper run would lead to the following new
time period:</p>
<div class="highlight-python"><pre>new next action factor (NAF) := 10 (INITIAL_NEXT_ACTION_FACTOR) / 1.3 (FACTOR_CHANGE_FACTOR) = 7.69 (rounded)
time period after successful run := 15 Minutes * 7.69 (NAF) = 115 Minutes</pre>
</div>
<p>So if it turns out that your scraper always find new items the time period between two runs gets smaller and smaller
until the defined <tt class="docutils literal"><span class="pre">MIN_TIME</span></tt> is reached which is taken as a minimum time period between two scraper runs.
If your scraper was not successful (meaning, that no new items were found) these unsucessful actions (scraper runs)
are counted as <tt class="docutils literal"><span class="pre">ZERO_ACTIONS</span></tt>. If a number of unsuccessful actions greater than <tt class="docutils literal"><span class="pre">ZERO_ACTIONS_FACTOR_CHANGE</span></tt>
is counted, a new next action factor is calculated, this time by taking the product of the actual action factor
and the <tt class="docutils literal"><span class="pre">FACTOR_CHANGE_FACTOR</span></tt> (calculation restarting from initial values for the example):</p>
<div class="highlight-python"><pre>new next action factor (NAF) := 10 (INITIAL_NEXT_ACTION_FACTOR) * 1.3 (FACTOR_CHANGE_FACTOR) = 13
time period after 21 unsuccessful runs := 15 Minutes * 13 (NAF) = 195 Minutes</pre>
</div>
<p>So the time period between two scraper runs becomes larger. If there is never a new item found for your scraper
this will go on until the calculated time period reaches the <tt class="docutils literal"><span class="pre">MAX_TIME</span></tt> defined.</p>
<p>In the real world application of this mechanism normally neither the <tt class="docutils literal"><span class="pre">MIN_TIME</span></tt> nor the <tt class="docutils literal"><span class="pre">MAX_TIME</span></tt> should be
reached. The normal situation is that your scraper often finds nothing new on the page to be scraped and than
after x executed runs finds new items provided on the website to be scraped. If this x is generally lower than
your defined <tt class="docutils literal"><span class="pre">ZERO_ACTIONS_FACTOR_CHANGE</span></tt> number, the time period is becoming shorter over time. But since this
means more scraper runs in the same time chances are high that with these narrower scheduled
runs less zero actions occur and leads at some point to an again increased next action factor. So some kind of
(relatively) stable next action factor should be reached over time, representing in the best case a good compromise
between the needs of actuality of your scrapers and not to much resources wasted on running your scraper
on websites not updated in between two runs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Since this is a relatively complex mechanism also depending on a large part on the update process of your
scraped website, it will probably take some time to get a bit a feeling for how the scheduling is developing
and to what action factors it tends to, so don&#8217;t try to find the perfect solution in the first run. Instead,
start with a (maybe rather too conservatively calculated) start configuration and adjust your parameters over
time. You can observe the development of your action factors in the scheduler runtime objects.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please be aware that scraping is a resource consuming task, for your server but as well for the server of
the websites you are scraping. Try to find a balanced solution, not just setting your MIN_TIME to 1 minute
or similar.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you don&#8217;t need dynamic scheduling, you can also just set the MIN_TIME and the MAX_TIME to the same
values and just ignore the rest.</p>
</div>
<p>Scheduling of your checkers works very similar to the scraper scheduling, the inital configuration is as follows:</p>
<div class="highlight-python"><pre>"MIN_TIME": 1440,
"MAX_TIME": 10080,
"INITIAL_NEXT_ACTION_FACTOR": 1,
"ZERO_ACTIONS_FACTOR_CHANGE": 5,
"FACTOR_CHANGE_FACTOR": 1.3,</pre>
</div>
<p>Since the checker scheduling is terminated with the success of a checker run (meaning the item and the associated
scheduler runtime is deleted), there is only the prolonging time period part of the scheduler actually working.
So scraped items are checked in a (relatively, defined by your configuration) short time period at first.
If the item turns out to be persistently existing, the checks are prolonged till <tt class="docutils literal"><span class="pre">MAX_TIME</span></tt> is reached.</p>
</div>
</div>
<div class="section" id="pagination">
<h2>Pagination<a class="headerlink" href="#pagination" title="Permalink to this headline">¶</a></h2>
<p>Django Dynamic Scraper supports pagination for scraping your objects from several overview pages or archives.
The following screenshot shows the pagination parameters which can be defined in the Django admin
for each scraper:</p>
<img alt="_images/screenshot_django-admin_pagination.png" src="_images/screenshot_django-admin_pagination.png" />
<p>For using pagination you have to switch the <tt class="docutils literal"><span class="pre">pagination_type</span></tt> in your scraper definition from <tt class="docutils literal"><span class="pre">NONE</span></tt> to
your desired type. The main concept of pagination is, that you define a <tt class="docutils literal"><span class="pre">pagination_append_str</span></tt> with a
placeholder <tt class="docutils literal"><span class="pre">{page}</span></tt>, which is replaced through a list generated by selecting the <tt class="docutils literal"><span class="pre">pagination_type</span></tt> and
giving a corresponding <tt class="docutils literal"><span class="pre">pagination_page_replace</span></tt> context. There are the following pagination types to
choose from:</p>
<div class="section" id="pagination-type-range-funct">
<h3>Pagination type: RANGE_FUNCT<a class="headerlink" href="#pagination-type-range-funct" title="Permalink to this headline">¶</a></h3>
<p>This pagination type uses the <a class="reference external" href="http://docs.python.org/library/functions.html#range">python range function</a>.
As a replace context the same arguments like in the range function are used: <tt class="docutils literal"><span class="pre">range([start],</span> <span class="pre">stop[,</span> <span class="pre">step])</span></tt>.
The integer list created by this function will be used as an input to replace the &#8220;{page}&#8221; template tag in the
append string to form the different urls.</p>
<p>So the parameters in our example above in the screenshot will lead - together with &#8220;<a class="reference external" href="http://www.urltoscrape.org">http://www.urltoscrape.org</a>&#8221;
as the base scrape url of your scraper runtime - to the following urls to be scraped:</p>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.urltoscrape.org/articles/0">http://www.urltoscrape.org/articles/0</a></li>
<li><a class="reference external" href="http://www.urltoscrape.org/articles/10">http://www.urltoscrape.org/articles/10</a></li>
<li><a class="reference external" href="http://www.urltoscrape.org/articles/20">http://www.urltoscrape.org/articles/20</a></li>
<li><a class="reference external" href="http://www.urltoscrape.org/articles/30">http://www.urltoscrape.org/articles/30</a></li>
</ol>
</div>
<div class="section" id="pagination-type-free-list">
<h3>Pagination type: FREE_LIST<a class="headerlink" href="#pagination-type-free-list" title="Permalink to this headline">¶</a></h3>
<p>If the urls from an archive are formed differently you can use this pagination type and just provide a list
with different fitting replacements, the syntax is as follow: <tt class="docutils literal"><span class="pre">'Replace</span> <span class="pre">text</span> <span class="pre">1',</span> <span class="pre">'Some</span> <span class="pre">other</span> <span class="pre">text</span> <span class="pre">2',</span>
<span class="pre">'Maybe</span> <span class="pre">a</span> <span class="pre">number</span> <span class="pre">3',</span> <span class="pre">...</span></tt>.</p>
<p>So if you define a list as follows: <tt class="docutils literal"><span class="pre">'a-d',</span> <span class="pre">'e-h',</span> <span class="pre">'i-n',</span> <span class="pre">'o-z'</span></tt>, you get the following urls:</p>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.urltoscrape.org/articles/a-d">http://www.urltoscrape.org/articles/a-d</a></li>
<li><a class="reference external" href="http://www.urltoscrape.org/articles/e-h">http://www.urltoscrape.org/articles/e-h</a></li>
<li><a class="reference external" href="http://www.urltoscrape.org/articles/i-n">http://www.urltoscrape.org/articles/i-n</a></li>
<li><a class="reference external" href="http://www.urltoscrape.org/articles/o-z">http://www.urltoscrape.org/articles/o-z</a></li>
</ol>
</div>
</div>
<div class="section" id="scraping-images-screenshots">
<h2>Scraping images/screenshots<a class="headerlink" href="#scraping-images-screenshots" title="Permalink to this headline">¶</a></h2>
<p>Django Dynamic Scraper is providing a custom image pipeline build on Scrapy&#8217;s <a class="reference external" href="http://readthedocs.org/docs/scrapy/en/latest/topics/images.html">item pipeline for downloading
images</a> to scrape and download images
associated to your items scraped and and save a reference to each image together with the scraped item in the DB.</p>
<div class="section" id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h3>
<p>For using image scraping in DDS you have to provide some additional parameters in your Scrapy
<cite>settings.py</cite> file:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">os.path</span>

<span class="n">PROJECT_ROOT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>

<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">&#39;dynamic_scraper.pipelines.DjangoImagesPipeline&#39;</span><span class="p">,</span>
    <span class="s">&#39;dynamic_scraper.pipelines.ValidationPipeline&#39;</span><span class="p">,</span>
    <span class="s">&#39;open_news.scraper.pipelines.DjangoWriterPipeline&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">PROJECT_ROOT</span><span class="p">,</span> <span class="s">&#39;../thumbnails&#39;</span><span class="p">)</span>

<span class="n">IMAGES_THUMBS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;small&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">170</span><span class="p">,</span> <span class="mi">170</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In your settings file you have to add the <cite>DjangoImagesPipeline</cite> from DDS to your <cite>ITEM_PIPELINES</cite> and define
a folder to store images scraped. Don&#8217;t forget to create this folder in your file system and give it adequate
permissions. You can also use the thumbnail creation capabilities already build in Scrapy
by defining the thumbnail size via the <cite>IMAGE_THUMBS</cite> parameter. The semantics here is a bit different from
what you would expect from Scrapy: thumbnail images in DDS are not stored in addition to the original images
but are replacing them completely and there is also no extra folder created for them. So when you use  the
<cite>IMAGE_THUMBS</cite> parameter in DDS the image scraping and storing process stays exacly the same but instead with
the original images you are ending up with images scaled down to the defined size. Due to this simplification
you can only use one entry in your <cite>IMAGES_THUMBS</cite> dictionary and the name of the key there doesn&#8217;t matter.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For image scraping to work you need the <a class="reference external" href="http://www.pythonware.com/products/pil/">Python Image Library (PIL)</a>.</p>
</div>
</div>
<div class="section" id="updating-domain-model-class-scraped-obj-class-definition">
<h3>Updating domain model class/scraped obj class definition<a class="headerlink" href="#updating-domain-model-class-scraped-obj-class-definition" title="Permalink to this headline">¶</a></h3>
<p>When Scrapy is downloading images it creates a new unique random file name for each image saved in your image
folder defined above. To keep a reference to the image associated with a scraped item DDS will save this filename
in a field you have to define in your model class. In our open news example, we use &#8216;thumbnail&#8217; as a field name:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">Article</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">CharField</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">news_website</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">NewsWebsite</span><span class="p">)</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">TextField</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">thumbnail</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">CharField</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">checker_runtime</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">SchedulerRuntime</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__unicode__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">title</span>
</pre></div>
</div>
<p>Note, that since there is just the filename of the image saved, you should declare this field as a simple
CharField and not using UrlField or ImageField.</p>
<p>Now you have to update your <a class="reference internal" href="reference.html#scraped-obj-class"><em>ScrapedObjClass</em></a> definition in the Django admin interface. Add a new attribute
with the same name like in your model class and choose <cite>IMAGE</cite> as the attribute type. <cite>IMAGE</cite> is a special
type to let your scraper know, that the image pipeline of DDS should be used when scraping this attribute.</p>
</div>
<div class="section" id="extending-testing-the-scraper">
<h3>Extending/Testing the scraper<a class="headerlink" href="#extending-testing-the-scraper" title="Permalink to this headline">¶</a></h3>
<p>At last we have to add a new scraper elem to our scraper, again in the Django admin interface, which scrapes and
builds together the url of the image for the image pipeline to download later. Let&#8217;s have a look at the <a class="reference external" href="http://en.wikinews.org/wiki/Main_Page">Wikinews</a>
website of our open news example. On the news article overview page there is also an image presented with each
article summary, which we want to scrape. <tt class="docutils literal"><span class="pre">div[&#64;class=&quot;l_image&quot;]/a/img/&#64;src</span></tt> should provide us with the url
of that image. Since the image urls we scrape with our XPath are starting with a double slash &#8216;//&#8217; and not with
&#8216;<a class="reference external" href="http://">http://</a>&#8216;, we also have to use a pre_url processor with <tt class="docutils literal"><span class="pre">'pre_url':</span> <span class="pre">'http:'</span></tt> as the processor context to
complete the url.</p>
<p>That&#8217;s it! If you now run your scraper, you should see lines like the following in the output (if you are in
debug mode) and you should end up with the images saved in your defined images folder and the names of these
images stored in the image field of your domain model in the DB:</p>
<div class="highlight-python"><pre>DEBUG: Image (downloaded): Downloaded image from &lt;GET http://upload.wikimedia.org/wikipedia/commons/thumb/...
...
u'thumbnail': '1bb3308a4c70b912ba6cf9d67344bb53476d70a2.jpg',</pre>
</div>
<p>So now you have all these images, but how to rid of them if you don&#8217;t need them any more? If you use
a checker to delete scraped items not existing any more, your images will be automatically deleted as well.
However, if you manually delete scraped items in your database, you have to delete the associated file yourself.</p>
</div>
</div>
<div class="section" id="where-to-go-from-here">
<h2>Where to go from here<a class="headerlink" href="#where-to-go-from-here" title="Permalink to this headline">¶</a></h2>
<p>So now that you have got your scraper up and running and maybe even integrated some of the advanced stuff
like pagination or scraping images, does that mean that life will become boring because there is nothing
to be done left? Definitely not! Here are some ideas about what to do next:</p>
<ul class="simple">
<li>Contribute to Django Dynamic Scraper through the experiences you made while using it (see <a class="reference internal" href="development.html#contribute"><em>How to contribute</em></a>)</li>
<li>Make your scraped data searchable with <a class="reference external" href="http://haystacksearch.org/">Django Haystack</a></li>
<li>Provide an API to your scraped data so that others can use it with <a class="reference external" href="https://github.com/toastdriven/django-tastypie">Django Tastypie</a></li>
<li>Or... just do something no one has ever done before! :-)</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Advanced topics</a><ul>
<li><a class="reference internal" href="#defining-item-checkers">Defining item checkers</a><ul>
<li><a class="reference internal" href="#creating-a-checker-class">Creating a checker class</a></li>
<li><a class="reference internal" href="#select-checker-type-set-check-parameters">Select checker type/set check parameters</a></li>
<li><a class="reference internal" href="#running-your-checkers">Running your checkers</a></li>
<li><a class="reference internal" href="#run-checker-tests">Run checker tests</a></li>
</ul>
</li>
<li><a class="reference internal" href="#scheduling-scrapers-checkers">Scheduling scrapers/checkers</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#installing-configuring-django-celery-for-dds">Installing/configuring django-celery for DDS</a></li>
<li><a class="reference internal" href="#defining-your-tasks">Defining your tasks</a></li>
<li><a class="reference internal" href="#run-your-tasks">Run your tasks</a></li>
<li><a class="reference internal" href="#scheduling-configuration">Scheduling configuration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pagination">Pagination</a><ul>
<li><a class="reference internal" href="#pagination-type-range-funct">Pagination type: RANGE_FUNCT</a></li>
<li><a class="reference internal" href="#pagination-type-free-list">Pagination type: FREE_LIST</a></li>
</ul>
</li>
<li><a class="reference internal" href="#scraping-images-screenshots">Scraping images/screenshots</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#updating-domain-model-class-scraped-obj-class-definition">Updating domain model class/scraped obj class definition</a></li>
<li><a class="reference internal" href="#extending-testing-the-scraper">Extending/Testing the scraper</a></li>
</ul>
</li>
<li><a class="reference internal" href="#where-to-go-from-here">Where to go from here</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="getting_started.html"
                        title="previous chapter">Getting started</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="basic_services.html"
                        title="next chapter">Basic services</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/advanced_topics.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="basic_services.html" title="Basic services"
             >next</a> |</li>
        <li class="right" >
          <a href="getting_started.html" title="Getting started"
             >previous</a> |</li>
        <li><a href="index.html">django-dynamic-scraper 0.2-alpha documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Holger Drewes.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>